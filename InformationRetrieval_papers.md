# Detailed Summary of Key Information Retrieval Papers

| Paper/Resource | Core Concept | Model Architecture | Base Model(s) / Size | Key Technique(s) | Training Data/Strategy | RAG / Multimodality |
|:---|:---|:---|:---|:---|:---|:---|
| **[Qwen3 Embedding](https://arxiv.org/abs/2506.05176)** | Multilingual text embedding & reranking with Qwen3 models; SOTA on retrieval benchmarks | Bi-Encoder, Cross-Encoder | Qwen3: 0.6B, 4B, 8B (embedding & reranking) | Multi-stage training (unsupervised + supervised), model merging, instruction-aware prompting, MRL, synthetic data | 3-stage: contrastive pretrain, supervised finetune, model merge; uses Qwen3 LLMs for multilingual data | RAG-ready, 100+ languages (incl. code), #1 MTEB Multilingual, supports code/text retrieval, classification, clustering |
| **[Fanar](https://fanar.qa/en)** | An Arabic-centric, multimodal generative AI platform with custom LLMs, tokenizer, and specialized services | Decoder-only Transformer | **Fanar Star:** 7B (trained from scratch)<br>**Fanar Prime:** 9B (continual training on Gemma-2 9B) | Custom morphology-aware Arabic tokenizer<br>Multi-stage curriculum pre-training<br>Orchestrator to route prompts | ~1 Trillion token dataset (40-50% Arabic, 40-50% English, 10% Code) | **RAG:** Islamic, Recency, Biography, and Attribution RAGs<br>**Multimodal:** Bilingual Speech Recognition (ASR/TTS) and culturally-aligned Image Generation |
| **[DRAGON: How to Train Your DRAGON](https://arxiv.org/abs/2302.07452)** | General dense retriever training for strong supervised and zero-shot performance | Bi-Encoder Dense Retriever | BERT-base (110M) | **Data Augmentation:** Cropped sentences, GenQ<br>**Label Augmentation:** Multiple retriever teachers (sparse, dense, multi-vector) | Augmented MS MARCO data only | Text-only dense retriever; |
| **[MM-Embed: Universal Multimodal Retrieval with Multimodal LLMs](https://arxiv.org/abs/2411.02571)** | A universal multimodal retriever fine-tuned from an MLLM to handle diverse tasks with multimodal queries and documents | Bi-Encoder Retriever | LLaVa-Next (Mistral 7B) | **Modality-Aware Hard Negative Mining:** Mitigates MLLM's bias towards text<br>**Continuous Fine-Tuning:** Enhances text retrieval while preserving multimodal capabilities<br>**Prompting for Reranking:** Uses frozen MLLM as a zero-shot reranker | Fine-tuned on M-BEIR (16 multimodal tasks) and various text retrieval datasets (MTEB) | **Core Multimodal Retriever:** Handles text, image, and interleaved text-image queries and documents |
| **[Pretrained Transformers for Text Ranking: BERT and Beyond](https://arxiv.org/abs/2010.06467)** (Survey/Book) | Survey of text ranking from BM25 to dense retrievers (BERT) | Cross-Encoder (Reranking), Bi-Encoder (Dense Retrieval) | Focus on **BERT-base** (110M), **BERT-large** (340M); also covers **RoBERTa**, **ELECTRA**, **T5** | **Reranking:** monoBERT, BERT-MaxP, CEDR, PARADE, monoT5<br>**Dense Retrieval:** DPR, ANCE, ColBERT<br>**Refinement:** doc2query, DeepCT | Mainly **MS MARCO**. Discusses pre-train/fine-tune, multi-step fine-tuning, and negative sampling (in-batch, hard negatives) | **Text-only.** Focuses on retrieval (not generation) and handling long documents with transformers |