# Comprehensive LLM Prompting Techniques

A collection of methodologies for LLM prompts that can be used to improve the retrieval of documents.

| **Category** | **Technique** | **Seminal Paper(s)** | **Core Idea** | **Key Strengths** | **Primary Limitations** |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Foundational** | **Zero-Shot Prompting** | Brown et al., 2020 | Directly instruct the model to perform a task without providing any examples. | Simple, versatile, requires no example data. | Performance can be unreliable for complex or nuanced tasks. |
| | **Few-Shot Prompting** | Brown et al., 2020 | Provide a small number of input-output demonstrations ("shots") within the prompt to guide the model. | Significantly improves accuracy and task alignment by showing the desired format and logic. | Requires manual creation of high-quality examples; increases prompt length and cost. |
| | **Retrieval-Augmented Generation (RAG)** | Lewis et al., 2020 | Retrieve relevant information from an external knowledge base and provide it as context to the LLM before generation. | Reduces hallucinations, allows access to up-to-date or proprietary information, can provide sources. | Performance is highly dependent on the quality of the retrieval system; can add latency. |
| **Structured Reasoning (Decomposition)** | **Chain-of-Thought (CoT)** | Wei et al., 2022; Kojima et al., 2022 | Prompt the model to generate a series of intermediate, step-by-step reasoning traces before arriving at a final answer. | Dramatically improves performance on complex reasoning tasks (math, logic); enhances interpretability. | Prone to error propagation (one mistake derails the chain); requires very large models to be effective. |
| | **Least-to-Most Prompting** | Zhou et al., 2022 | Decompose a complex problem into a sequence of simpler subproblems and solve them sequentially, using previous answers as context. | Effective for problems that have a clear compositional structure; reduces the complexity of each step. | Requires the ability to define and order the subproblems; less flexible than a freeform CoT. |
| | **Step-Back Prompting** | Zheng et al., 2023 | Instruct the LLM to first "step back" from a specific, detailed question to derive a high-level concept or principle. | Grounds reasoning in correct fundamentals; prevents the model from getting lost in misleading details. | Often requires manual design of the abstract "step-back" questions for the specific domain. |
| | **Thread of Thought (ThoT)** | Zhou et al., 2023 | A specialized CoT variant to maintain a coherent line of reasoning across long or "chaotic" contexts (e.g., RAG, long dialogues). | Ensures key details are not overlooked in long contexts; improves focus and structure. | Can be overly verbose; specialized for long-context or retrieval-heavy tasks. |
| **Structured Reasoning (Verification & Refinement)** | **Self-Consistency** | Wang et al., 2022 | Sample multiple, diverse reasoning paths for the same problem and select the most consistent final answer via a majority vote. | Significantly increases accuracy and robustness by marginalizing out flawed reasoning paths. | High computational and token cost due to generating multiple full reasoning chains. |
| | **Self-Refine** | Madaan et al., 2023 | Use an iterative process where the LLM generates an output, then provides feedback on its own output, and then refines it. | Improves output quality without needing supervised data or reinforcement learning; creates an autonomous feedback loop. | Refinement is not guaranteed to be an improvement; can be computationally expensive. |
| | **Chain-of-Verification (CoVe)** | Dhuliawala et al., 2023 | Explicitly prompt the model to plan verifications, execute them, and use the results to generate a final, verified answer. | Directly targets the problem of factual hallucination by introducing a verification step. | Adds significant complexity and latency to the generation process. |
| **Search-based Exploration** | **Tree of Thoughts (ToT)** | Yao et al., 2023 | Generalize CoT by allowing the LLM to explore multiple reasoning paths concurrently in a tree structure, with self-evaluation and backtracking. | Enables solving complex planning and search problems where linear reasoning (CoT) fails. | Very high computational cost; performance is critically dependent on the LLM's self-evaluation ability. |
| | **Graph-of-Thoughts (GoT)** | Besta et al., 2023 | A further generalization of ToT where thoughts are modeled as a graph, allowing paths to be merged and more complex dependencies to be modeled. | More flexible and powerful than ToT, capable of representing richer reasoning structures. | Even more complex and computationally intensive than ToT; practical implementation is challenging. |
| **Agentic & Tool-Using Frameworks** | **ReAct (Reason + Act)** | Yao et al., 2022 | Prompt the model to interleave verbal reasoning traces (Thought) with actions that interact with external tools (Action). | Enables grounding in external reality, overcoming knowledge cutoffs, and completing tasks via APIs. | Performance is dependent on the reliability of the tools; can be slow due to sequential tool calls. |
| | **Program-Aided Language Models (PAL)** | Gao et al., 2022 | Prompt the LLM to write code (e.g., Python) to solve a problem, then execute the code to get the answer. | Offloads complex calculation and symbolic logic to a reliable interpreter, drastically reducing reasoning errors. | Limited to problems that can be solved programmatically. |
| | **ART (Automatic Reasoning and Tool-use)** | Paranjape et al., 2023 | From a few demonstrations, the LLM learns to select tools from a library and generate the necessary inputs to solve a new task. | Automates the decomposition of a task into a sequence of tool-using steps. | Performance is limited by the quality and scope of the provided tool library and demonstrations. |
| | **MM-ReAct** | Yang et al., 2023 | Extends the ReAct framework to multimodal contexts, allowing an LLM to call upon specialized vision models as tools. | Synergizes language reasoning with expert visual understanding to solve complex multimodal tasks. | Requires a suite of available vision models; adds significant system complexity. |
| **Automated Prompt Design** | **Automatic Prompt Engineer (APE)** | Zhou et al., 2022 | Use an LLM to automatically generate and select the best natural language instruction for a given task based on performance. | Reduces the manual, often intuitive, effort of prompt engineering. | The search process for the best prompt is computationally expensive and can yield suboptimal results. |
| | **Auto-CoT** | Zhang et al., 2022 | Automates the creation of few-shot CoT demonstrations by clustering questions and using Zero-Shot-CoT to generate rationales. | Eliminates the labor-intensive and skill-based process of manually writing high-quality CoT exemplars. | The quality of the automatically generated reasoning chains can be inconsistent. |
| | **Optimization by PROmpting (OPRO)** | Yang et al., 2023 | Uses an LLM to iteratively refine prompts in an evolutionary-like process, using the history of attempts to guide improvement. | Can discover highly optimized and non-intuitive prompts that outperform human-designed ones. | The optimization loop is very computationally intensive. |